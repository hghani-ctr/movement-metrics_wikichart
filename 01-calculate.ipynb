{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "import wmfdata as wmf\n",
    "from wmfdata.utils import print_err, pd_display_all\n",
    "\n",
    "from src.utils import load_metric_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the date parameters are determined from these two:\n",
    "# * metrics_month_text (e.g. \"2023-08\"): the month metrics are generated for\n",
    "# * mediawiki_history_snapshot (e.g. \"2023-08\"): the version of mediawiki_history\n",
    "#   used to generate the editing metrics. This should generally be the latest available,\n",
    "#   even if you are not generating metrics for the latest month.\n",
    "#\n",
    "# Both key parameters are generated automatically by assuming they are the last completed\n",
    "# month, but you can manually set them to different values if necessary.\n",
    "last_month = datetime.date.today().replace(day=1) - datetime.timedelta(days=1)\n",
    "\n",
    "metrics_month_text = last_month.strftime(\"%Y-%m\")\n",
    "mediawiki_history_snapshot = metrics_month_text\n",
    "\n",
    "\n",
    "# Convert our two date parameters to all the formats we need and provide them in a dict\n",
    "# so we can easily use them to format strings\n",
    "metrics_month = pd.Period(metrics_month_text)\n",
    "\n",
    "date_params = {\n",
    "    \"api_metrics_month_first_day\": metrics_month.asfreq(\"D\", how=\"start\").strftime(\"%Y%m%d\"),\n",
    "    \"api_metrics_month_day_after\": (metrics_month + 1).asfreq(\"D\", how=\"start\").strftime(\"%Y%m%d\"),\n",
    "    \"mediawiki_history_snapshot\": mediawiki_history_snapshot,\n",
    "    \"metrics_cur_month\": metrics_month.month,\n",
    "    \"metrics_month\": str(metrics_month),\n",
    "    \"metrics_month_first_day\": str(metrics_month.asfreq(\"D\", how=\"start\")),\n",
    "    \"metrics_month_end\": str((metrics_month + 1).start_time),\n",
    "    \"metrics_month_last_day\": str(metrics_month.asfreq(\"D\", how=\"end\")),\n",
    "    \"metrics_month_start\": str(metrics_month.start_time),\n",
    "    \"metrics_next_month_first_day\": str((metrics_month + 1).asfreq(\"D\", how=\"start\")),\n",
    "    \"metrics_prev_month\": str(metrics_month - 1),\n",
    "    \"metrics_year\": metrics_month.year,\n",
    "    \"retention_cohort\": str(metrics_month - 2)\n",
    "}\n",
    "\n",
    "def prepare_query(filename):\n",
    "    return (\n",
    "        Path(filename)\n",
    "        .read_text()\n",
    "        .format(**date_params)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricSet:\n",
    "    \"\"\"\n",
    "    A MetricSet is a group of monthly metrics that is saved to a single file and\n",
    "    which is generated by one or more queries.\n",
    "    \n",
    "    Class assumptions:\n",
    "    * Each query contains a \"month\" column which Pandas can parse into a date.\n",
    "    * The column names used in the queries are unique across queries.\n",
    "    \n",
    "    A note on the date formats:\n",
    "    The month column is saved to the file as the ISO-8601 date of the start of the\n",
    "    month (e.g. 2020-06-01). However, in Python we handle them not as Datetimes but as\n",
    "    Periods, since this makes it easy to write code (e.g. utils.calc_rpt) which works\n",
    "    both with the normal monthly as well as quarterly aggregates.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, filename, queries):\n",
    "        self.filename = filename\n",
    "        self.queries = queries\n",
    "        self.load_data()\n",
    "\n",
    "    def load_data(self):\n",
    "        try:\n",
    "            self.data = load_metric_file(self.filename)\n",
    "        except FileNotFoundError:\n",
    "            self.data = pd.DataFrame()\n",
    "\n",
    "    def add_data(self, new_data):\n",
    "        \"\"\"\n",
    "        Takes a Pandas data frame with a date index giving the month and one or more\n",
    "        columns of metrics.\n",
    "        \"\"\"\n",
    "        self.data = new_data.combine_first(self.data)\n",
    "\n",
    "    def run_queries(self):\n",
    "        for key, val in self.queries.items():\n",
    "            query = prepare_query(val[\"file\"])\n",
    "            print_err(f\"Running {key} \")\n",
    "\n",
    "            result = wmf.spark.run(query)\n",
    "\n",
    "            result = (\n",
    "                result\n",
    "                .assign(month=lambda df: pd.to_datetime(df[\"month\"]))\n",
    "                .set_index(\"month\")\n",
    "                .to_period()\n",
    "            )\n",
    "\n",
    "            self.add_data(result)\n",
    "\n",
    "    def save_data(self):\n",
    "        self.data.to_timestamp().to_csv(self.filename, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Editing metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running active_editors \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SPARK_HOME: /usr/lib/spark3\n",
      "Using Hadoop client lib jars at 3.2.0, provided by Spark.\n",
      "PYSPARK_PYTHON=/opt/conda-analytics/bin/python3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/10/06 01:08:38 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n",
      "23/10/06 01:08:38 WARN Utils: Service 'sparkDriver' could not bind on port 12000. Attempting port 12001.\n",
      "23/10/06 01:08:38 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/10/06 01:08:46 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 13000. Attempting port 13001.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 20\u001b[0m\n\u001b[1;32m      1\u001b[0m editing_queries \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactive_editors\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqueries/active_editors.sql\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m     }\n\u001b[1;32m     17\u001b[0m }\n\u001b[1;32m     19\u001b[0m editing_metrics \u001b[38;5;241m=\u001b[39m MetricSet(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetrics/editing_metrics.tsv\u001b[39m\u001b[38;5;124m\"\u001b[39m, editing_queries)\n\u001b[0;32m---> 20\u001b[0m \u001b[43mediting_metrics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_queries\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [3], line 31\u001b[0m, in \u001b[0;36mMetricSet.run_queries\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     28\u001b[0m query \u001b[38;5;241m=\u001b[39m prepare_query(val[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     29\u001b[0m print_err(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 31\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mwmf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m result \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     34\u001b[0m     result\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;241m.\u001b[39massign(month\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m df: pd\u001b[38;5;241m.\u001b[39mto_datetime(df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmonth\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;241m.\u001b[39mset_index(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmonth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;241m.\u001b[39mto_period()\n\u001b[1;32m     38\u001b[0m )\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_data(result)\n",
      "File \u001b[0;32m/srv/home/nshahquinn-wmf/wmfdata-python/wmfdata/spark.py:277\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(commands)\u001b[0m\n\u001b[1;32m    275\u001b[0m session \u001b[38;5;241m=\u001b[39m get_active_session()\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m session:\n\u001b[0;32m--> 277\u001b[0m     session \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m    279\u001b[0m overall_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cmd \u001b[38;5;129;01min\u001b[39;00m commands:\n",
      "File \u001b[0;32m/srv/home/nshahquinn-wmf/wmfdata-python/wmfdata/spark.py:244\u001b[0m, in \u001b[0;36mcreate_session\u001b[0;34m(type, app_name, extra_settings, ship_python_env)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;66;03m# Add in any extra settings, overwriting if applicable\u001b[39;00m\n\u001b[1;32m    242\u001b[0m config\u001b[38;5;241m.\u001b[39mupdate(extra_settings)\n\u001b[0;32m--> 244\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcreate_custom_session\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaster\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPREDEFINED_SPARK_SESSIONS\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaster\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapp_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapp_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspark_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m    \u001b[49m\u001b[43mship_python_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mship_python_env\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/home/nshahquinn-wmf/wmfdata-python/wmfdata/spark.py:202\u001b[0m, in \u001b[0;36mcreate_custom_session\u001b[0;34m(master, app_name, spark_config, ship_python_env, conda_pack_kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m spark_config\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    200\u001b[0m     builder\u001b[38;5;241m.\u001b[39mconfig(k, v)\n\u001b[0;32m--> 202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/2023-09-18T23.55.38_nshahquinn-wmf/lib/python3.10/site-packages/pyspark/sql/session.py:228\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    226\u001b[0m         sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[0;32m--> 228\u001b[0m     sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[1;32m    231\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc)\n",
      "File \u001b[0;32m~/.conda/envs/2023-09-18T23.55.38_nshahquinn-wmf/lib/python3.10/site-packages/pyspark/context.py:384\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 384\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[0;32m~/.conda/envs/2023-09-18T23.55.38_nshahquinn-wmf/lib/python3.10/site-packages/pyspark/context.py:146\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    144\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway\u001b[38;5;241m=\u001b[39mgateway, conf\u001b[38;5;241m=\u001b[39mconf)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mappName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparkHome\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpyFiles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatchSize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjsc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofiler_cls\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;66;03m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[39;00m\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop()\n",
      "File \u001b[0;32m~/.conda/envs/2023-09-18T23.55.38_nshahquinn-wmf/lib/python3.10/site-packages/pyspark/context.py:209\u001b[0m, in \u001b[0;36mSparkContext._do_init\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvironment[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    208\u001b[0m \u001b[38;5;66;03m# Create the Java SparkContext through Py4J\u001b[39;00m\n\u001b[0;32m--> 209\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc \u001b[38;5;241m=\u001b[39m jsc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_context\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;66;03m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conf \u001b[38;5;241m=\u001b[39m SparkConf(_jconf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc()\u001b[38;5;241m.\u001b[39mconf())\n",
      "File \u001b[0;32m~/.conda/envs/2023-09-18T23.55.38_nshahquinn-wmf/lib/python3.10/site-packages/pyspark/context.py:321\u001b[0m, in \u001b[0;36mSparkContext._initialize_context\u001b[0;34m(self, jconf)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_initialize_context\u001b[39m(\u001b[38;5;28mself\u001b[39m, jconf):\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;124;03m    Initialize SparkContext in function to allow subclass specific initialization\u001b[39;00m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 321\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mJavaSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjconf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/spark3/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:1567\u001b[0m, in \u001b[0;36mJavaClass.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1559\u001b[0m args_command \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m   1560\u001b[0m     [get_command_part(arg, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m new_args])\n\u001b[1;32m   1562\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCONSTRUCTOR_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1563\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_command_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1564\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1565\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1567\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1568\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1569\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fqn)\n\u001b[1;32m   1571\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/usr/lib/spark3/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:1033\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1031\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1033\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1034\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1035\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/usr/lib/spark3/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:1200\u001b[0m, in \u001b[0;36mGatewayConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JNetworkError(\n\u001b[1;32m   1197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError while sending\u001b[39m\u001b[38;5;124m\"\u001b[39m, e, proto\u001b[38;5;241m.\u001b[39mERROR_ON_SEND)\n\u001b[1;32m   1199\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1200\u001b[0m     answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m   1201\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m   1202\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m answer\u001b[38;5;241m.\u001b[39mstartswith(proto\u001b[38;5;241m.\u001b[39mRETURN_MESSAGE):\n",
      "File \u001b[0;32m~/.conda/envs/2023-09-18T23.55.38_nshahquinn-wmf/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/06 01:08:46 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to request executors before the AM has registered!\n"
     ]
    }
   ],
   "source": [
    "editing_queries = {\n",
    "    \"active_editors\": {\n",
    "        \"file\": \"queries/active_editors.sql\"\n",
    "    },\n",
    "    \"edits\": {\n",
    "        \"file\": \"queries/edits.sql\"\n",
    "    },\n",
    "    \"new_editor_retention\": {\n",
    "        \"file\": \"queries/new_editor_retention.sql\"\n",
    "    },\n",
    "    \"mobile-heavy_edits_editors\": {\n",
    "        \"file\": \"queries/mobile-heavy_edits_editors.sql\"\n",
    "    },\n",
    "    \"mobile-heavy_new_editor_retention\": {\n",
    "        \"file\": \"queries/mobile-heavy_new_editor_retention.sql\"\n",
    "    }\n",
    "}\n",
    "\n",
    "editing_metrics = MetricSet(\"metrics/editing_metrics.tsv\", editing_queries)\n",
    "editing_metrics.run_queries()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content metrics via API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_PAGES_API = (\n",
    "    \"https://wikimedia.org/api/rest_v1/metrics/\"\n",
    "    \"edited-pages/new/{project}/all-editor-types/{page_type}/monthly/{start}/{end}\"\n",
    ")\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"https://github.com/wikimedia-research/movement-metrics (bot)\"\n",
    "}\n",
    "\n",
    "api_results = []\n",
    "\n",
    "def get_new_pages(\n",
    "    project=\"all-projects\",\n",
    "    page_type=\"content\",\n",
    "    start=date_params[\"api_metrics_month_first_day\"],\n",
    "    end=date_params[\"api_metrics_month_day_after\"]\n",
    "):\n",
    "    url = NEW_PAGES_API.format(\n",
    "        project = project,\n",
    "        page_type = page_type,\n",
    "        start = start,\n",
    "        end = end\n",
    "    )\n",
    "\n",
    "    r = requests.get(url, headers=headers)\n",
    "    data = r.json()[\"items\"][0][\"results\"]\n",
    "    frame = pd.DataFrame(data)\n",
    "    frame[\"timestamp\"] = pd.to_datetime(frame[\"timestamp\"]).dt.tz_localize(None)\n",
    "    frame = (\n",
    "        frame\n",
    "        .rename(columns={\"timestamp\": \"month\"})\n",
    "        .set_index(\"month\")\n",
    "    )\n",
    "\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_new = (\n",
    "    get_new_pages()\n",
    "    .rename(columns={\"new_pages\": \"net_new_content_pages\"})\n",
    ")\n",
    "\n",
    "editing_metrics.add_data(total_new)\n",
    "\n",
    "\n",
    "wikidata_new = (\n",
    "    get_new_pages(project=\"wikidata.org\")\n",
    "    .rename(columns={\"new_pages\": \"net_new_Wikidata_entities\"})\n",
    ")\n",
    "\n",
    "editing_metrics.add_data(wikidata_new)\n",
    "\n",
    "\n",
    "commons_new = (\n",
    "    get_new_pages(project=\"commons.wikimedia.org\")\n",
    "    .rename(columns={\"new_pages\": \"net_new_Commons_content_pages\"})\n",
    ")\n",
    "\n",
    "editing_metrics.add_data(commons_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Now on project 50 of 335 (ceb.wikipedia.org)                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Now on project 100 of 335 (glk.wikipedia.org)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Now on project 150 of 335 (kj.wikipedia.org)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Now on project 200 of 335 (mus.wikipedia.org)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Now on project 250 of 335 (roa-tara.wikipedia.org)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Now on project 300 of 335 (tpi.wikipedia.org)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "wp_domains = wmf.spark.run(\"\"\"\n",
    "    SELECT domain_name\n",
    "    FROM canonical_data.wikis\n",
    "    WHERE database_group = \"wikipedia\"\n",
    "\"\"\")[\"domain_name\"]\n",
    "\n",
    "results = []\n",
    "n = len(wp_domains)\n",
    "\n",
    "for i, domain in enumerate(wp_domains):\n",
    "    p = i + 1\n",
    "\n",
    "    if p % 50 == 0:\n",
    "        print_err(f\"Now on project {p} of {n} ({domain})\")\n",
    "\n",
    "    frame = get_new_pages(project=domain)\n",
    "    frame[\"project\"] = domain\n",
    "    results.append(frame)\n",
    "\n",
    "    # Be polite to the API\n",
    "    time.sleep(0.02)\n",
    "\n",
    "new_per_wp = pd.concat(results)\n",
    "\n",
    "# Sum across projects to get new Wikipedia articles per month\n",
    "wikipedia_new = (\n",
    "    new_per_wp\n",
    "    .groupby(\"month\")\n",
    "    .agg({\"new_pages\": \"sum\"})\n",
    "    .rename(columns={\"new_pages\": \"net_new_Wikipedia_articles\"})\n",
    ")\n",
    "\n",
    "editing_metrics.add_data(wikipedia_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "editing_metrics.save_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Readers metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running pageviews on hive...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running automated_pageviews on hive...                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running page_previews on hive...                                                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running unique_devices on hive...                                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "readers_queries = {\n",
    "    \"pageviews\": {\n",
    "        \"file\": \"queries/pageviews.sql\"\n",
    "    },\n",
    "    \"automated_pageviews\": {\n",
    "        \"file\": \"queries/automated_pageviews.sql\"\n",
    "    },\n",
    "    \"page_previews\": {\n",
    "        \"file\": \"queries/page_previews.sql\"\n",
    "    },\n",
    "    \"unique_devices\": {\n",
    "        \"file\": \"queries/unique_devices.sql\"\n",
    "    }\n",
    "}\n",
    "\n",
    "readers_metrics = MetricSet(\"metrics/readers_metrics.tsv\", readers_queries)\n",
    "readers_metrics.run_queries()\n",
    "readers_metrics.data = (\n",
    "    readers_metrics.data\n",
    "    .assign(interactions=lambda df: df[\"previews_seen\"] + df[\"total_pageview\"])\n",
    ")\n",
    "readers_metrics.save_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running regional_unique_devices \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SPARK_HOME: /usr/lib/spark3\n",
      "Using Hadoop client lib jars at 3.2.0, provided by Spark.\n",
      "PYSPARK_PYTHON=/opt/conda-analytics/bin/python3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/10/04 00:35:16 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n",
      "23/10/04 00:35:16 WARN Utils: Service 'sparkDriver' could not bind on port 12000. Attempting port 12001.\n",
      "23/10/04 00:35:16 WARN Utils: Service 'sparkDriver' could not bind on port 12001. Attempting port 12002.\n",
      "23/10/04 00:35:16 WARN Utils: Service 'sparkDriver' could not bind on port 12002. Attempting port 12003.\n",
      "23/10/04 00:35:16 WARN Utils: Service 'sparkDriver' could not bind on port 12003. Attempting port 12004.\n",
      "23/10/04 00:35:16 WARN Utils: Service 'sparkDriver' could not bind on port 12004. Attempting port 12005.\n",
      "23/10/04 00:35:17 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/10/04 00:35:17 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "23/10/04 00:35:17 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "23/10/04 00:35:17 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "23/10/04 00:35:17 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n",
      "23/10/04 00:35:26 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 13000. Attempting port 13001.\n",
      "23/10/04 00:35:26 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 13001. Attempting port 13002.\n",
      "23/10/04 00:35:26 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 13002. Attempting port 13003.\n",
      "23/10/04 00:35:26 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 13003. Attempting port 13004.\n",
      "23/10/04 00:35:26 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 13004. Attempting port 13005.\n",
      "23/10/04 00:35:26 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to request executors before the AM has registered!\n",
      "23/10/04 00:35:32 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "regional_unique_devices_queries = {\n",
    "    \"regional_unique_devices\": {\n",
    "        \"file\": \"queries/regional_unique_devices.sql\"\n",
    "    }\n",
    "}\n",
    "\n",
    "regional_unique_devices = MetricSet(\"metrics/regional_unique_devices.tsv\", regional_unique_devices_queries)\n",
    "regional_unique_devices.run_queries()\n",
    "regional_unique_devices.save_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Editing diversity metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running global_south_edits_editors on hive...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running global_south_new_editor_retention on hive...                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running global_south_net_new_content on hive...                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running global_south_wikidata_entities on hive...                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running global_north_edits_editors on hive...                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running global_north_new_editor_retention on hive...                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running global_north_net_new_content on hive...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running global_north_wikidata_entities on hive...                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "editing_diversity_queries = {\n",
    "    \"global_south_edits_editors\": {\n",
    "        \"file\": \"queries/global_south_edits_editors.sql\"\n",
    "    },\n",
    "    \"global_south_new_editor_retention\": {\n",
    "        \"file\": \"queries/global_south_new_editor_retention.sql\"\n",
    "    },\n",
    "    \"global_south_net_new_content\": {\n",
    "        \"file\": \"queries/global_south_net_new_content.sql\"\n",
    "    },\n",
    "    \"global_south_wikidata_entities\": {\n",
    "        \"file\": \"queries/global_south_net_new_wikidata.sql\"\n",
    "    },\n",
    "    \"global_north_edits_editors\": {\n",
    "        \"file\": \"queries/global_north_edits_editors.sql\"\n",
    "    },\n",
    "    \"global_north_new_editor_retention\": {\n",
    "        \"file\": \"queries/global_north_new_editor_retention.sql\"\n",
    "    },\n",
    "    \"global_north_net_new_content\": {\n",
    "        \"file\": \"queries/global_north_net_new_content.sql\"\n",
    "    },\n",
    "    \"global_north_wikidata_entities\": {\n",
    "        \"file\": \"queries/global_north_net_new_wikidata.sql\"\n",
    "    }\n",
    "}\n",
    "\n",
    "editing_diversity_metrics = MetricSet(\"metrics/editing_diversity_metrics.tsv\", editing_diversity_queries)\n",
    "editing_diversity_metrics.run_queries()\n",
    "editing_diversity_metrics.save_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running global_south_pageviews on hive...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running global_south_previews on hive...                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running global_north_previews on hive...                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running global_north_pageviews on hive...                                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "readers_diversity_queries = {\n",
    "    \"global_south_pageviews\": {\n",
    "        \"file\": \"queries/global_south_pageviews.sql\"\n",
    "    },\n",
    "    \"global_south_previews\": {\n",
    "        \"file\": \"queries/global_south_previews.sql\"\n",
    "    },\n",
    "    \"global_north_previews\": {\n",
    "        \"file\": \"queries/global_north_previews.sql\"\n",
    "    },\n",
    "     \"global_north_pageviews\": {\n",
    "        \"file\": \"queries/global_north_pageviews.sql\"\n",
    "    }\n",
    "}\n",
    "\n",
    "readers_diversity_metrics = MetricSet(\"metrics/readers_diversity_metrics.tsv\", readers_diversity_queries)\n",
    "readers_diversity_metrics.run_queries()\n",
    "\n",
    "readers_diversity_metrics.data = (\n",
    "    readers_diversity_metrics.data\n",
    "    .assign(\n",
    "        gs_interactions=lambda df: df[\"gs_previews\"] + df[\"gs_pageviews\"],\n",
    "        gn_interactions=lambda df: df[\"gn_previews\"] + df[\"gn_pageviews\"]\n",
    "    )\n",
    ")\n",
    "\n",
    "readers_diversity_metrics.save_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
