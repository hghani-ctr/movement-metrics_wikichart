{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from functools import reduce\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "import wmfdata\n",
    "from wmfdata import hive, spark\n",
    "from wmfdata.utils import print_err, pd_display_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_month = datetime.date.today().replace(day=1) - datetime.timedelta(days=1)\n",
    "METRICS_MONTH_TEXT = last_month.strftime(\"%Y-%m\")\n",
    "MEDIAWIKI_HISTORY_SNAPSHOT = last_month.strftime(\"%Y-%m\")\n",
    "metrics_month = pd.Period(METRICS_MONTH_TEXT)\n",
    "\n",
    "# Convert our metrics month to all the formats we need and provide them in a dict\n",
    "# so we can easily use them to format strings\n",
    "date_params = {\n",
    "    \"api_metrics_month_first_day\": metrics_month.asfreq(\"D\", how=\"start\").strftime(\"%Y%m%d\"),\n",
    "    \"api_metrics_month_day_after\": (metrics_month + 1).asfreq(\"D\", how=\"start\").strftime(\"%Y%m%d\"),\n",
    "    \"mediawiki_history_snapshot\": MEDIAWIKI_HISTORY_SNAPSHOT,\n",
    "    \"metrics_cur_month\": last_month.month,\n",
    "    \"metrics_month\": str(metrics_month),\n",
    "    \"metrics_month_first_day\": str(metrics_month.asfreq(\"D\", how=\"start\")),\n",
    "    \"metrics_month_end\": str((metrics_month + 1).start_time),\n",
    "    \"metrics_month_last_day\": str(metrics_month.asfreq(\"D\", how=\"end\")),\n",
    "    \"metrics_month_start\": str(metrics_month.start_time),\n",
    "    \"metrics_next_month_first_day\": str((metrics_month + 1).asfreq(\"D\", how=\"start\")),\n",
    "    \"metrics_prev_month\": str(metrics_month - 1),\n",
    "    \"metrics_year\": last_month.year,\n",
    "    \"retention_cohort\": str(metrics_month - 2)\n",
    "}\n",
    "\n",
    "def prepare_query(filename):\n",
    "    return (\n",
    "        Path(filename)\n",
    "        .read_text()\n",
    "        .format(**date_params)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricSet:\n",
    "    \"\"\"\n",
    "    Class assumptions:\n",
    "    * Each query contains a \"month\" column which Pandas can parse into a date.\n",
    "    * The column names used in the queries are unique across queries.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, filename, queries):\n",
    "        self.filename = filename\n",
    "        self.queries = queries\n",
    "        self.load_data()\n",
    "\n",
    "    def load_data(self):\n",
    "        try:\n",
    "            self.data = (\n",
    "                pd\n",
    "                .read_csv(self.filename, sep=\"\\t\", parse_dates = [\"month\"])\n",
    "                .set_index(\"month\")\n",
    "            )\n",
    "        except FileNotFoundError:\n",
    "            self.data = pd.DataFrame()\n",
    "\n",
    "    def add_data(self, new_data):\n",
    "        \"\"\"\n",
    "        Takes a Pandas data frame with a date index giving the month and one or more\n",
    "        columns of metrics.\n",
    "        \"\"\"\n",
    "        self.data = new_data.combine_first(self.data)\n",
    "\n",
    "    def run_queries(self):\n",
    "        for key, val in self.queries.items():\n",
    "            query = prepare_query(val[\"file\"])\n",
    "            engine = val[\"engine\"]\n",
    "            print_err(\"Running {} on {}...\".format(key, engine))\n",
    "\n",
    "            if engine == \"mariadb\":\n",
    "                result = mariadb.run(query)\n",
    "            elif engine == \"hive\":\n",
    "                result = spark.run(query)\n",
    "            else:\n",
    "                raise ValueError(\"Unknown engine specified.\") \n",
    "\n",
    "            result = (\n",
    "                result\n",
    "                .assign(month=lambda df: pd.to_datetime(df[\"month\"]))\n",
    "                .set_index(\"month\")\n",
    "            )\n",
    "            \n",
    "            self.add_data(result)\n",
    "\n",
    "    def save_data(self):\n",
    "        self.data.to_csv(self.filename, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Editing metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running active_editors on hive...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running edits on hive...                                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running new_editor_retention on hive...                                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running global_south_edits_editors on hive...                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running mobile-heavy_edits_editors on hive...                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running mobile-heavy_new_editor_retention on hive...                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running global_south_new_editor_retention on hive...                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "editing_queries = {\n",
    "    \"active_editors\": {\n",
    "        \"file\": \"queries/active_editors.sql\",\n",
    "        \"engine\": \"hive\"\n",
    "    },\n",
    "    \"edits\": {\n",
    "        \"file\": \"queries/edits.hql\",\n",
    "        \"engine\": \"hive\"\n",
    "    },\n",
    "    \"new_editor_retention\": {\n",
    "        \"file\": \"queries/new_editor_retention.hql\",\n",
    "         \"engine\": \"hive\"\n",
    "    },\n",
    "    \"global_south_edits_editors\": {\n",
    "        \"file\": \"queries/global_south_edits_editors.hql\",\n",
    "        \"engine\": \"hive\"\n",
    "    },\n",
    "    \"mobile-heavy_edits_editors\": {\n",
    "        \"file\": \"queries/mobile-heavy_edits_editors.hql\",\n",
    "        \"engine\": \"hive\"\n",
    "    },\n",
    "    \"mobile-heavy_new_editor_retention\": {\n",
    "        \"file\": \"queries/mobile-heavy_new_editor_retention.hql\",\n",
    "        \"engine\": \"hive\"\n",
    "    },\n",
    "    \"global_south_new_editor_retention\": {\n",
    "        \"file\": \"queries/global_south_new_editor_retention.hql\",\n",
    "        \"engine\": \"hive\"\n",
    "    }\n",
    "}\n",
    "\n",
    "editing_metrics = MetricSet(\"metrics/editing_metrics.tsv\", editing_queries)\n",
    "editing_metrics.run_queries()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content metrics via API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_PAGES_API = (\n",
    "    \"https://wikimedia.org/api/rest_v1/metrics/\"\n",
    "    \"edited-pages/new/{project}/all-editor-types/{page_type}/monthly/{start}/{end}\"\n",
    ")\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"https://github.com/wikimedia-research/movement-metrics (bot)\"\n",
    "}\n",
    "\n",
    "api_results = []\n",
    "\n",
    "def get_new_pages(\n",
    "    project=\"all-projects\",\n",
    "    page_type=\"content\",\n",
    "    start=date_params[\"api_metrics_month_first_day\"],\n",
    "    end=date_params[\"api_metrics_month_day_after\"]\n",
    "):\n",
    "    url = NEW_PAGES_API.format(\n",
    "        project = project,\n",
    "        page_type = page_type,\n",
    "        start = start,\n",
    "        end = end\n",
    "    )\n",
    "    \n",
    "    r = requests.get(url, headers=headers)\n",
    "    data = r.json()[\"items\"][0][\"results\"]\n",
    "    frame = pd.DataFrame(data)\n",
    "    frame[\"timestamp\"] = pd.to_datetime(frame[\"timestamp\"]).dt.tz_localize(None)\n",
    "    frame = (\n",
    "        frame\n",
    "        .rename(columns={\"timestamp\": \"month\"})\n",
    "        .set_index(\"month\")\n",
    "    )\n",
    "    \n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_new = (\n",
    "    get_new_pages()\n",
    "    .rename(columns={\"new_pages\": \"net_new_content_pages\"})\n",
    ")\n",
    "\n",
    "editing_metrics.add_data(total_new)\n",
    "\n",
    "\n",
    "wikidata_new = (\n",
    "    get_new_pages(project=\"wikidata.org\")\n",
    "    .rename(columns={\"new_pages\": \"net_new_Wikidata_entities\"})\n",
    ")\n",
    "\n",
    "editing_metrics.add_data(wikidata_new)\n",
    "\n",
    "\n",
    "commons_new = (\n",
    "    get_new_pages(project=\"commons.wikimedia.org\")\n",
    "    .rename(columns={\"new_pages\": \"net_new_Commons_content_pages\"})\n",
    ")\n",
    "\n",
    "editing_metrics.add_data(commons_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Now on project 50 of 335 (ceb.wikipedia.org)                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Now on project 100 of 335 (glk.wikipedia.org)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Now on project 150 of 335 (kj.wikipedia.org)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Now on project 200 of 335 (mus.wikipedia.org)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Now on project 250 of 335 (roa-tara.wikipedia.org)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Now on project 300 of 335 (tpi.wikipedia.org)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "wp_domains = spark.run(\"\"\"\n",
    "    SELECT domain_name\n",
    "    FROM canonical_data.wikis\n",
    "    WHERE database_group = \"wikipedia\"\n",
    "\"\"\")[\"domain_name\"]\n",
    "\n",
    "results = []\n",
    "n = len(wp_domains)\n",
    "\n",
    "for i, domain in enumerate(wp_domains):\n",
    "    p = i + 1\n",
    "    \n",
    "    if p % 50 == 0:\n",
    "        print_err(f\"Now on project {p} of {n} ({domain})\")\n",
    "        \n",
    "    frame = get_new_pages(project=domain)\n",
    "    frame[\"project\"] = domain\n",
    "    results.append(frame)\n",
    "    \n",
    "    # Be polite to the API\n",
    "    time.sleep(0.02)\n",
    "\n",
    "new_per_wp = pd.concat(results)\n",
    "\n",
    "# Sum across projects to get new Wikipedia articles per month\n",
    "wikipedia_new = (\n",
    "    new_per_wp\n",
    "    .groupby(\"month\")\n",
    "    .agg({\"new_pages\": \"sum\"})\n",
    "    .rename(columns={\"new_pages\": \"net_new_Wikipedia_articles\"})\n",
    ")\n",
    "\n",
    "editing_metrics.add_data(wikipedia_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "editing_metrics.save_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Readers metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running pageviews on hive...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running automated_pageviews on hive...                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running page_previews on hive...                                                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running unique_devices on hive...                                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "readers_queries = {\n",
    "    \"pageviews\": {\n",
    "        \"file\": \"queries/pageviews.hql\",\n",
    "        \"engine\": \"hive\"\n",
    "    },\n",
    "    \"automated_pageviews\": {\n",
    "        \"file\": \"queries/automated_pageviews.hql\",\n",
    "        \"engine\": \"hive\"\n",
    "    },\n",
    "    \"page_previews\": {\n",
    "        \"file\": \"queries/page_previews.hql\",\n",
    "        \"engine\": \"hive\"\n",
    "    },\n",
    "    \"unique_devices\": {\n",
    "        \"file\": \"queries/unique_devices.hql\",\n",
    "        \"engine\": \"hive\"\n",
    "    }     \n",
    "}\n",
    "\n",
    "readers_metrics = MetricSet(\"metrics/readers_metrics.tsv\", readers_queries)\n",
    "readers_metrics.run_queries()\n",
    "readers_metrics.data = (\n",
    "    readers_metrics.data\n",
    "    .assign(interactions=lambda df: df[\"previews_seen\"] + df[\"total_pageview\"])\n",
    ")\n",
    "readers_metrics.save_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Editing diversity metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running global_south_edits_editors on hive...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running global_south_new_editor_retention on hive...                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running global_south_net_new_content on hive...                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running global_south_wikidata_entities on hive...                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running global_north_edits_editors on hive...                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running global_north_new_editor_retention on hive...                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running global_north_net_new_content on hive...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running global_north_wikidata_entities on hive...                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "editing_diversity_queries = {\n",
    "    \"global_south_edits_editors\": {\n",
    "        \"file\": \"queries/global_south_edits_editors.hql\",\n",
    "        \"engine\": \"hive\"\n",
    "    },\n",
    "\n",
    "    \"global_south_new_editor_retention\": {\n",
    "        \"file\": \"queries/global_south_new_editor_retention.hql\",\n",
    "        \"engine\": \"hive\"\n",
    "    },\n",
    "    \n",
    "    \"global_south_net_new_content\": {\n",
    "        \"file\": \"queries/global_south_net_new_content.hql\",\n",
    "        \"engine\": \"hive\"\n",
    "    },\n",
    "    \n",
    "    \"global_south_wikidata_entities\": {\n",
    "        \"file\": \"queries/global_south_net_new_wikidata.hql\",\n",
    "        \"engine\": \"hive\"\n",
    "    },\n",
    "    \n",
    "    \"global_north_edits_editors\": {\n",
    "        \"file\": \"queries/global_north_edits_editors.hql\",\n",
    "        \"engine\": \"hive\"\n",
    "    },\n",
    "\n",
    "    \"global_north_new_editor_retention\": {\n",
    "        \"file\": \"queries/global_north_new_editor_retention.hql\",\n",
    "        \"engine\": \"hive\"\n",
    "    },\n",
    "    \n",
    "    \"global_north_net_new_content\": {\n",
    "        \"file\": \"queries/global_north_net_new_content.hql\",\n",
    "        \"engine\": \"hive\"\n",
    "    },\n",
    "    \"global_north_wikidata_entities\": {\n",
    "        \"file\": \"queries/global_north_net_new_wikidata.hql\",\n",
    "        \"engine\": \"hive\"\n",
    "    }\n",
    "}\n",
    "\n",
    "editing_diversity_metrics = MetricSet(\"metrics/editing_diversity_metrics.tsv\", editing_diversity_queries)\n",
    "editing_diversity_metrics.run_queries()\n",
    "editing_diversity_metrics.save_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running global_south_pageviews on hive...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running global_south_previews on hive...                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running global_north_previews on hive...                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running global_north_pageviews on hive...                                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "readers_diversity_queries = {\n",
    "    \"global_south_pageviews\": {\n",
    "        \"file\": \"queries/global_south_pageviews.hql\",\n",
    "        \"engine\": \"hive\"\n",
    "    },\n",
    "    \"global_south_previews\": {\n",
    "        \"file\": \"queries/global_south_previews.hql\",\n",
    "        \"engine\": \"hive\"\n",
    "    },\n",
    "    \"global_north_previews\": {\n",
    "        \"file\": \"queries/global_north_previews.hql\",\n",
    "        \"engine\": \"hive\"\n",
    "    },\n",
    "     \"global_north_pageviews\": {\n",
    "        \"file\": \"queries/global_north_pageviews.hql\",\n",
    "        \"engine\": \"hive\"\n",
    "    }   \n",
    "}\n",
    "\n",
    "readers_diversity_metrics = MetricSet(\"metrics/readers_diversity_metrics.tsv\", readers_diversity_queries)\n",
    "readers_diversity_metrics.run_queries()\n",
    "\n",
    "readers_diversity_metrics.data = (\n",
    "    readers_diversity_metrics.data\n",
    "    .assign(\n",
    "        gs_interactions=lambda df: df[\"gs_previews\"] + df[\"gs_pageviews\"],\n",
    "        gn_interactions=lambda df: df[\"gn_previews\"] + df[\"gn_pageviews\"]\n",
    "    )\n",
    ")\n",
    "\n",
    "readers_diversity_metrics.save_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
