{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "225cfc53-ff54-41d4-9a64-bbe2d97f17d9",
   "metadata": {},
   "source": [
    "# Reader data for regional views\n",
    "[T331359](https://phabricator.wikimedia.org/T331359)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619cdc02-a56f-4be4-910c-fcbd35d0304a",
   "metadata": {},
   "source": [
    "## Unique Devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cbc22cc-15d5-48d5-88ee-509d3cc254ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wmfdata import spark, hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3b2fa10-18ec-4ee3-bd5f-ba6c35df93ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wmfdata as wmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd6e1583-cdf0-40c7-834b-1a5da0806075",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_devices_query='''\n",
    "SELECT\n",
    "    DATE_FORMAT(month, 'YYYY-MM') AS month,\n",
    "    SUM(unique_devices) AS unique_devices\n",
    "FROM\n",
    "(\n",
    "    SELECT\n",
    "      CONCAT(year,'-',LPAD(month,2,'0'),'-01') AS month,\n",
    "      uniques_estimate as unique_devices\n",
    "    FROM \n",
    "        wmf.unique_devices_per_project_family_monthly\n",
    "    WHERE \n",
    "        year = 2023\n",
    "      AND project_family = 'wikipedia'\n",
    ") a\n",
    "\n",
    "GROUP BY DATE_FORMAT(month, 'YYYY-MM') \n",
    "LIMIT 1000\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a1e5796-6a56-415d-8f9e-b1ff06119048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPARK_HOME: /usr/lib/spark3\n",
      "Using Hadoop client lib jars at 3.2.0, provided by Spark.\n",
      "PYSPARK_PYTHON=/opt/conda-analytics/bin/python3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/09/21 23:09:51 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n",
      "23/09/21 23:09:51 WARN Utils: Service 'sparkDriver' could not bind on port 12000. Attempting port 12001.\n",
      "23/09/21 23:09:51 WARN Utils: Service 'sparkDriver' could not bind on port 12001. Attempting port 12002.\n",
      "23/09/21 23:09:52 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/09/21 23:09:52 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "23/09/21 23:10:01 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 13000. Attempting port 13001.\n",
      "23/09/21 23:10:01 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 13001. Attempting port 13002.\n",
      "23/09/21 23:10:01 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to request executors before the AM has registered!\n",
      "23/09/21 23:10:05 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o101.collectToPython.\n: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange hashpartitioning(date_format(cast(month#0 as timestamp), YYYY-MM, Some(Etc/UTC))#23, 256), ENSURE_REQUIREMENTS, [id=#33]\n+- *(1) HashAggregate(keys=[date_format(cast(month#0 as timestamp), YYYY-MM, Some(Etc/UTC)) AS date_format(cast(month#0 as timestamp), YYYY-MM, Some(Etc/UTC))#23], functions=[partial_sum(cast(unique_devices#1 as bigint))], output=[date_format(cast(month#0 as timestamp), YYYY-MM, Some(Etc/UTC))#23, sum#25L])\n   +- *(1) Project [concat(cast(year#10 as string), -, lpad(cast(month#11 as string), 2, 0), -01) AS month#0, uniques_estimate#9 AS unique_devices#1]\n      +- *(1) Filter (isnotnull(project_family#4) AND (project_family#4 = wikipedia))\n         +- *(1) ColumnarToRow\n            +- FileScan parquet wmf.unique_devices_per_project_family_monthly[project_family#4,uniques_estimate#9,year#10,month#11] Batched: true, DataFilters: [isnotnull(project_family#4), (project_family#4 = wikipedia)], Format: Parquet, Location: InMemoryFileIndex[hdfs://analytics-hadoop/wmf/data/wmf/unique_devices/per_project_family/monthly/..., PartitionFilters: [isnotnull(year#10), (year#10 = 2023)], PushedFilters: [IsNotNull(project_family), EqualTo(project_family,wikipedia)], ReadSchema: struct<project_family:string,uniques_estimate:int>\n\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:163)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:141)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:746)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:321)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:439)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3519)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3516)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkUpgradeException: You may get a different result due to the upgrading of Spark 3.0: Fail to recognize 'YYYY-MM' pattern in the DateTimeFormatter. 1) You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0. 2) You can form a valid datetime pattern with the guide from https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkLegacyFormatter$1.applyOrElse(DateTimeFormatterHelper.scala:196)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkLegacyFormatter$1.applyOrElse(DateTimeFormatterHelper.scala:185)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.validatePatternString(TimestampFormatter.scala:109)\n\tat org.apache.spark.sql.catalyst.util.TimestampFormatter$.getFormatter(TimestampFormatter.scala:300)\n\tat org.apache.spark.sql.catalyst.util.TimestampFormatter$.apply(TimestampFormatter.scala:333)\n\tat org.apache.spark.sql.catalyst.expressions.TimestampFormatterHelper.getFormatter(datetimeExpressions.scala:72)\n\tat org.apache.spark.sql.catalyst.expressions.TimestampFormatterHelper.getFormatter$(datetimeExpressions.scala:67)\n\tat org.apache.spark.sql.catalyst.expressions.DateFormatClass.getFormatter(datetimeExpressions.scala:772)\n\tat org.apache.spark.sql.catalyst.expressions.TimestampFormatterHelper.$anonfun$formatterOption$1(datetimeExpressions.scala:64)\n\tat scala.Option.map(Option.scala:230)\n\tat org.apache.spark.sql.catalyst.expressions.TimestampFormatterHelper.formatterOption(datetimeExpressions.scala:64)\n\tat org.apache.spark.sql.catalyst.expressions.TimestampFormatterHelper.formatterOption$(datetimeExpressions.scala:62)\n\tat org.apache.spark.sql.catalyst.expressions.DateFormatClass.formatterOption$lzycompute(datetimeExpressions.scala:772)\n\tat org.apache.spark.sql.catalyst.expressions.DateFormatClass.formatterOption(datetimeExpressions.scala:772)\n\tat org.apache.spark.sql.catalyst.expressions.DateFormatClass.doGenCode(datetimeExpressions.scala:791)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$genCode$3(Expression.scala:146)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:141)\n\tat org.apache.spark.sql.catalyst.expressions.Alias.genCode(namedExpressions.scala:163)\n\tat org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.$anonfun$generateExpressions$1(CodeGenerator.scala:1187)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.generateExpressions(CodeGenerator.scala:1187)\n\tat org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection$.createCode(GenerateUnsafeProjection.scala:290)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.doConsumeWithKeys(HashAggregateExec.scala:827)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.doConsume(HashAggregateExec.scala:156)\n\tat org.apache.spark.sql.execution.CodegenSupport.constructDoConsumeFunction(WholeStageCodegenExec.scala:221)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:192)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:149)\n\tat org.apache.spark.sql.execution.ProjectExec.consume(basicPhysicalOperators.scala:41)\n\tat org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:87)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:194)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:149)\n\tat org.apache.spark.sql.execution.FilterExec.consume(basicPhysicalOperators.scala:113)\n\tat org.apache.spark.sql.execution.FilterExec.doConsume(basicPhysicalOperators.scala:238)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:194)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:149)\n\tat org.apache.spark.sql.execution.ColumnarToRowExec.consume(Columnar.scala:66)\n\tat org.apache.spark.sql.execution.ColumnarToRowExec.doProduce(Columnar.scala:191)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.ColumnarToRowExec.produce(Columnar.scala:66)\n\tat org.apache.spark.sql.execution.FilterExec.doProduce(basicPhysicalOperators.scala:153)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.FilterExec.produce(basicPhysicalOperators.scala:113)\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:54)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:41)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduceWithKeys(HashAggregateExec.scala:733)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduce(HashAggregateExec.scala:148)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.produce(HashAggregateExec.scala:47)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:655)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:718)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD$lzycompute(ShuffleExchangeExec.scala:118)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD(ShuffleExchangeExec.scala:118)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:151)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:149)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.$anonfun$doExecute$1(ShuffleExchangeExec.scala:166)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\t... 41 more\nCaused by: java.lang.IllegalArgumentException: All week-based patterns are unsupported since Spark 3.0, detected: Y, Please use the SQL function EXTRACT instead\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$.$anonfun$convertIncompatiblePattern$4(DateTimeFormatterHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$.$anonfun$convertIncompatiblePattern$4$adapted(DateTimeFormatterHelper.scala:321)\n\tat scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:877)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.immutable.StringOps.foreach(StringOps.scala:33)\n\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:876)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$.$anonfun$convertIncompatiblePattern$2(DateTimeFormatterHelper.scala:321)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$.convertIncompatiblePattern(DateTimeFormatterHelper.scala:318)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper.getOrCreateFormatter(DateTimeFormatterHelper.scala:121)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper.getOrCreateFormatter$(DateTimeFormatterHelper.scala:117)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.getOrCreateFormatter(TimestampFormatter.scala:59)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.formatter$lzycompute(TimestampFormatter.scala:68)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.formatter(TimestampFormatter.scala:67)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.validatePatternString(TimestampFormatter.scala:108)\n\t... 127 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m uniques\u001b[38;5;241m=\u001b[39m\u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43munique_devices_query\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/2023-09-21T22.51.44_osefu/lib/python3.10/site-packages/wmfdata/spark.py:290\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(commands)\u001b[0m\n\u001b[1;32m    287\u001b[0m         overall_result \u001b[38;5;241m=\u001b[39m cmd_result\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m overall_result:\n\u001b[0;32m--> 290\u001b[0m     overall_result \u001b[38;5;241m=\u001b[39m \u001b[43moverall_result\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m overall_result\n",
      "File \u001b[0;32m~/.conda/envs/2023-09-21T22.51.44_osefu/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:141\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m# Below is toPandas without Arrow optimization.\u001b[39;00m\n\u001b[0;32m--> 141\u001b[0m pdf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_records(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m    142\u001b[0m column_counter \u001b[38;5;241m=\u001b[39m Counter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m    144\u001b[0m dtype \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema)\n",
      "File \u001b[0;32m~/.conda/envs/2023-09-21T22.51.44_osefu/lib/python3.10/site-packages/pyspark/sql/dataframe.py:677\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m    668\u001b[0m \n\u001b[1;32m    669\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[38;5;124;03m[Row(age=2, name='Alice'), Row(age=5, name='Bob')]\u001b[39;00m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc) \u001b[38;5;28;01mas\u001b[39;00m css:\n\u001b[0;32m--> 677\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(PickleSerializer())))\n",
      "File \u001b[0;32m/usr/lib/spark3/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/.conda/envs/2023-09-21T22.51.44_osefu/lib/python3.10/site-packages/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/lib/spark3/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o101.collectToPython.\n: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange hashpartitioning(date_format(cast(month#0 as timestamp), YYYY-MM, Some(Etc/UTC))#23, 256), ENSURE_REQUIREMENTS, [id=#33]\n+- *(1) HashAggregate(keys=[date_format(cast(month#0 as timestamp), YYYY-MM, Some(Etc/UTC)) AS date_format(cast(month#0 as timestamp), YYYY-MM, Some(Etc/UTC))#23], functions=[partial_sum(cast(unique_devices#1 as bigint))], output=[date_format(cast(month#0 as timestamp), YYYY-MM, Some(Etc/UTC))#23, sum#25L])\n   +- *(1) Project [concat(cast(year#10 as string), -, lpad(cast(month#11 as string), 2, 0), -01) AS month#0, uniques_estimate#9 AS unique_devices#1]\n      +- *(1) Filter (isnotnull(project_family#4) AND (project_family#4 = wikipedia))\n         +- *(1) ColumnarToRow\n            +- FileScan parquet wmf.unique_devices_per_project_family_monthly[project_family#4,uniques_estimate#9,year#10,month#11] Batched: true, DataFilters: [isnotnull(project_family#4), (project_family#4 = wikipedia)], Format: Parquet, Location: InMemoryFileIndex[hdfs://analytics-hadoop/wmf/data/wmf/unique_devices/per_project_family/monthly/..., PartitionFilters: [isnotnull(year#10), (year#10 = 2023)], PushedFilters: [IsNotNull(project_family), EqualTo(project_family,wikipedia)], ReadSchema: struct<project_family:string,uniques_estimate:int>\n\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:163)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:141)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:746)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:321)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:439)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3519)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3516)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkUpgradeException: You may get a different result due to the upgrading of Spark 3.0: Fail to recognize 'YYYY-MM' pattern in the DateTimeFormatter. 1) You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0. 2) You can form a valid datetime pattern with the guide from https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkLegacyFormatter$1.applyOrElse(DateTimeFormatterHelper.scala:196)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkLegacyFormatter$1.applyOrElse(DateTimeFormatterHelper.scala:185)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.validatePatternString(TimestampFormatter.scala:109)\n\tat org.apache.spark.sql.catalyst.util.TimestampFormatter$.getFormatter(TimestampFormatter.scala:300)\n\tat org.apache.spark.sql.catalyst.util.TimestampFormatter$.apply(TimestampFormatter.scala:333)\n\tat org.apache.spark.sql.catalyst.expressions.TimestampFormatterHelper.getFormatter(datetimeExpressions.scala:72)\n\tat org.apache.spark.sql.catalyst.expressions.TimestampFormatterHelper.getFormatter$(datetimeExpressions.scala:67)\n\tat org.apache.spark.sql.catalyst.expressions.DateFormatClass.getFormatter(datetimeExpressions.scala:772)\n\tat org.apache.spark.sql.catalyst.expressions.TimestampFormatterHelper.$anonfun$formatterOption$1(datetimeExpressions.scala:64)\n\tat scala.Option.map(Option.scala:230)\n\tat org.apache.spark.sql.catalyst.expressions.TimestampFormatterHelper.formatterOption(datetimeExpressions.scala:64)\n\tat org.apache.spark.sql.catalyst.expressions.TimestampFormatterHelper.formatterOption$(datetimeExpressions.scala:62)\n\tat org.apache.spark.sql.catalyst.expressions.DateFormatClass.formatterOption$lzycompute(datetimeExpressions.scala:772)\n\tat org.apache.spark.sql.catalyst.expressions.DateFormatClass.formatterOption(datetimeExpressions.scala:772)\n\tat org.apache.spark.sql.catalyst.expressions.DateFormatClass.doGenCode(datetimeExpressions.scala:791)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$genCode$3(Expression.scala:146)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:141)\n\tat org.apache.spark.sql.catalyst.expressions.Alias.genCode(namedExpressions.scala:163)\n\tat org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.$anonfun$generateExpressions$1(CodeGenerator.scala:1187)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.generateExpressions(CodeGenerator.scala:1187)\n\tat org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection$.createCode(GenerateUnsafeProjection.scala:290)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.doConsumeWithKeys(HashAggregateExec.scala:827)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.doConsume(HashAggregateExec.scala:156)\n\tat org.apache.spark.sql.execution.CodegenSupport.constructDoConsumeFunction(WholeStageCodegenExec.scala:221)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:192)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:149)\n\tat org.apache.spark.sql.execution.ProjectExec.consume(basicPhysicalOperators.scala:41)\n\tat org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:87)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:194)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:149)\n\tat org.apache.spark.sql.execution.FilterExec.consume(basicPhysicalOperators.scala:113)\n\tat org.apache.spark.sql.execution.FilterExec.doConsume(basicPhysicalOperators.scala:238)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:194)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:149)\n\tat org.apache.spark.sql.execution.ColumnarToRowExec.consume(Columnar.scala:66)\n\tat org.apache.spark.sql.execution.ColumnarToRowExec.doProduce(Columnar.scala:191)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.ColumnarToRowExec.produce(Columnar.scala:66)\n\tat org.apache.spark.sql.execution.FilterExec.doProduce(basicPhysicalOperators.scala:153)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.FilterExec.produce(basicPhysicalOperators.scala:113)\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:54)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:41)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduceWithKeys(HashAggregateExec.scala:733)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduce(HashAggregateExec.scala:148)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.produce(HashAggregateExec.scala:47)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:655)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:718)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD$lzycompute(ShuffleExchangeExec.scala:118)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD(ShuffleExchangeExec.scala:118)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:151)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:149)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.$anonfun$doExecute$1(ShuffleExchangeExec.scala:166)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\t... 41 more\nCaused by: java.lang.IllegalArgumentException: All week-based patterns are unsupported since Spark 3.0, detected: Y, Please use the SQL function EXTRACT instead\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$.$anonfun$convertIncompatiblePattern$4(DateTimeFormatterHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$.$anonfun$convertIncompatiblePattern$4$adapted(DateTimeFormatterHelper.scala:321)\n\tat scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:877)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.immutable.StringOps.foreach(StringOps.scala:33)\n\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:876)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$.$anonfun$convertIncompatiblePattern$2(DateTimeFormatterHelper.scala:321)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$.convertIncompatiblePattern(DateTimeFormatterHelper.scala:318)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper.getOrCreateFormatter(DateTimeFormatterHelper.scala:121)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper.getOrCreateFormatter$(DateTimeFormatterHelper.scala:117)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.getOrCreateFormatter(TimestampFormatter.scala:59)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.formatter$lzycompute(TimestampFormatter.scala:68)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.formatter(TimestampFormatter.scala:67)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.validatePatternString(TimestampFormatter.scala:108)\n\t... 127 more\n"
     ]
    }
   ],
   "source": [
    "uniques=spark.run(unique_devices_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cb5e38-de2f-4e3f-b175-aeec9e370741",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5e6f71-24ba-451d-9e5b-7e2ae4ee1d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT \"project_family\" AS \"project_family\",\n",
    "       \"country\" AS \"country\",\n",
    "       \"country_code\" AS \"country_code\",\n",
    "       \"uniques_underestimate\" AS \"uniques_underestimate\",\n",
    "       \"uniques_offset\" AS \"uniques_offset\",\n",
    "       \"uniques_estimate\" AS \"uniques_estimate\",\n",
    "       \"year\" AS \"year\",\n",
    "       \"month\" AS \"month\"\n",
    "FROM \"wmf\".\"unique_devices_per_project_family_monthly\"\n",
    "WHERE \"year\" = 2023\n",
    "  AND \"month\" = 2\n",
    "LIMIT 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8827a0b-a422-4b68-8c17-78894d913819",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e85ca7c-8153-4d9e-883e-b0022a4bb343",
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_q= spark.run(\n",
    "'''\n",
    "    SELECT\n",
    "      CONCAT(year,'-',LPAD(month,2,'0'),'-01') AS month,\n",
    "      cr.wmf_region AS region,\n",
    "      uniques_estimate as unique_devices\n",
    "    FROM \n",
    "        wmf.unique_devices_per_project_family_monthly ud\n",
    "    JOIN gdi.country_meta_data cr \n",
    "    ON ud.country_code=cr.country_code_iso_2\n",
    "    WHERE \n",
    "        year = 2023 AND month=2\n",
    "      AND project_family = 'wikipedia'\n",
    "      \n",
    "'''      \n",
    ")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a76628-00d5-4fa7-8eab-49e7a06662a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_q.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b949a46d-c048-444b-840e-255b9df06f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_q.unique_devices.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8881727-5c18-47a6-99a7-1d63c7132784",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd5f4c4-24d4-47f9-acea-2f138230c3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_devices_by_region_test_query='''\n",
    "SELECT\n",
    "    DATE_FORMAT(month, 'YYYY-MM') AS month,\n",
    "    region,\n",
    "    SUM(unique_devices) AS unique_devices\n",
    "FROM\n",
    "(\n",
    "        SELECT\n",
    "      CONCAT(year,'-',LPAD(month,2,'0'),'-01') AS month,\n",
    "      cr.wmf_region AS region,\n",
    "      uniques_estimate as unique_devices\n",
    "    FROM \n",
    "        wmf.unique_devices_per_project_family_monthly ud\n",
    "    LEFT JOIN gdi.country_meta_data cr \n",
    "    ON ud.country_code=cr.country_code_iso_2\n",
    "    WHERE \n",
    "        year = 2023 AND month=2\n",
    "      AND project_family = 'wikipedia'\n",
    ") a\n",
    "\n",
    "GROUP BY DATE_FORMAT(month, 'YYYY-MM') , region\n",
    "\n",
    "LIMIT 1000\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6d5dc3-62f0-4c1e-8c8e-77752b7129e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "by_region_test=spark.run(unique_devices_by_region_test_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cc2c09-7775-46d0-bb3d-983916a834c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "by_region_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114ccf3a-90f8-4ef5-b378-7ced40ad039d",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_region=spark.run('''\n",
    "SELECT distinct country_code, country, country_code_iso_2, \n",
    "canonical_country_name, wmf_region\n",
    "FROM wmf.unique_devices_per_project_family_monthly ud\n",
    "--JOIN gdi.country_meta_data cr \n",
    "--ON ud.country_code!=cr.country_code_iso_2\n",
    "--, gdi.country_meta_data cr \n",
    "--WHERE ud.country_code!=cr.country_code_iso_2\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342310e2-b43a-4baa-9d05-86fe2e5032a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_region=spark.run('''\n",
    "SELECT distinct country_code, country\n",
    "FROM \n",
    "wmf.unique_devices_per_project_family_monthly ud\n",
    "--gdi.country_meta_data cr\n",
    "WHERE\n",
    "country_code NOT IN \n",
    "--NOT EXISTS\n",
    "(\n",
    "SELECT distinct country_code\n",
    "--, country\n",
    "--, country_code_iso_2, canonical_country_name, wmf_region\n",
    "FROM wmf.unique_devices_per_project_family_monthly ud\n",
    "JOIN gdi.country_meta_data cr \n",
    "ON ud.country_code=cr.country_code_iso_2\n",
    ")\n",
    "\n",
    "--JOIN gdi.country_meta_data cr \n",
    "--ON ud.country_code!=cr.country_code_iso_2\n",
    "--, gdi.country_meta_data cr \n",
    "--WHERE ud.country_code!=cr.country_code_iso_2\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e022b0-0529-416f-9370-145c0bf9dbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101e087d-bf6a-435c-9d2a-e0e14fdf9dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_q1= spark.run(\n",
    "'''\n",
    "    SELECT\n",
    "      CONCAT(year,'-',LPAD(month,2,'0'),'-01') AS month,\n",
    "      country,\n",
    "      uniques_estimate as unique_devices\n",
    "    FROM \n",
    "        wmf.unique_devices_per_project_family_monthly ud\n",
    "    WHERE \n",
    "        year = 2023 AND month=2\n",
    "      AND project_family = 'wikipedia'\n",
    "      AND country='Unknown'\n",
    "      \n",
    "'''      \n",
    ")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0444e197-37f8-458e-bdbc-07ca3126de82",
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_q1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad86bd7c-5107-450c-8442-bb6dc276c6cf",
   "metadata": {},
   "source": [
    "Caveat: there are some unique devices that are from `Unknown` location in the unique_devices table which is causing mismatch in the totals, i.e. the total count of unique devices by region will be lesser than the overall count of unique devices. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59bf086-1483-4d6c-acc6-156fd37c7065",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_devices_by_region_query='''\n",
    "SELECT\n",
    "    DATE_FORMAT(month, 'YYYY-MM') AS month,\n",
    "    region,\n",
    "    SUM(unique_devices) AS unique_devices\n",
    "FROM\n",
    "(\n",
    "        SELECT\n",
    "      CONCAT(year,'-',LPAD(month,2,'0'),'-01') AS month,\n",
    "      cr.wmf_region AS region,\n",
    "      uniques_estimate as unique_devices\n",
    "    FROM \n",
    "        wmf.unique_devices_per_project_family_monthly ud\n",
    "    LEFT JOIN gdi.country_meta_data cr \n",
    "    ON ud.country_code=cr.country_code_iso_2\n",
    "    WHERE \n",
    "        year >= 2018\n",
    "      AND project_family = 'wikipedia'\n",
    ") a\n",
    "\n",
    "GROUP BY DATE_FORMAT(month, 'YYYY-MM') , region\n",
    "\n",
    "LIMIT 1000\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d4d920-03c4-4a74-8b3f-0ae5e5fc615f",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_devices_by_region=spark.run(unique_devices_by_region_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87857d59-0c0a-4a3f-93f8-b4a30e9e3965",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_devices_by_region.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ca6886-d653-43ed-a7e9-5842bdf414ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_devices_by_region.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70f614a-e8df-4520-97eb-788a7631ba90",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_devices_by_region.to_csv('unique_devices_by_region')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db69b916-0f79-47e0-86bd-9e4fe8af99bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#unique_devices_by_region.loc['region'].sum()\n",
    "#unique_devices_by_region['Sum'] = unique_devices_by_region.loc['r2':'r4',['None']].sum(axis = 1)\n",
    "unique_devices_by_region['Sum_Result'] = unique_devices_by_region.loc[0 : 1,[\"unique_devices\" ]].sum(axis = 1)\n",
    "#print\"\\nSumming some rows...\\n\",dataFrame\n",
    "unique_devices_by_region"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41efccbe-9d3d-4160-8679-fb46aa368ddd",
   "metadata": {},
   "source": [
    "# March 2023 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6d52ad-1e10-4229-8719-83a99529ee3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_devices_by_region_query_march='''\n",
    "SELECT\n",
    "    CONCAT(year,'-',LPAD(month,2,'0')) AS month,\n",
    "    region,\n",
    "    SUM(unique_devices) AS unique_devices\n",
    "FROM\n",
    "(\n",
    "        SELECT\n",
    "      year, month,\n",
    "      cr.wmf_region AS region,\n",
    "      uniques_estimate as unique_devices\n",
    "    FROM \n",
    "        wmf.unique_devices_per_project_family_monthly ud\n",
    "    LEFT JOIN gdi.country_meta_data cr \n",
    "    ON ud.country_code=cr.country_code_iso_2\n",
    "    WHERE \n",
    "        year = 2023\n",
    "      AND month = 3\n",
    "      AND project_family = 'wikipedia'\n",
    ") a\n",
    "\n",
    "GROUP BY CONCAT(year,'-',LPAD(month,2,'0')) , region\n",
    "\n",
    "LIMIT 1000\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5dc5e2-d5c4-4d9e-9980-c7016b9a8439",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_devices_by_region_march=spark.run(unique_devices_by_region_query_march)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be37acb6-12b3-451a-9c2d-67ffffb66027",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_devices_by_region_march"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a36a82-c942-4822-b393-2a9b3cfd7a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wmfdata as wmf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d33d201-8fa9-4000-905f-e124a7d9a8cd",
   "metadata": {},
   "source": [
    "# April-May 2023 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad49c0e5-07a7-4e56-9493-6b5bd1dce7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_devices_by_region_query_aprilmay='''\n",
    "SELECT\n",
    "    CONCAT(year,'-',LPAD(month,2,'0')) AS month,\n",
    "    region,\n",
    "    SUM(unique_devices) AS unique_devices\n",
    "FROM\n",
    "(\n",
    "        SELECT\n",
    "      year, month,\n",
    "      cr.wmf_region AS region,\n",
    "      uniques_estimate as unique_devices\n",
    "    FROM \n",
    "        wmf.unique_devices_per_project_family_monthly ud\n",
    "    LEFT JOIN gdi.country_meta_data cr \n",
    "    ON ud.country_code=cr.country_code_iso_2\n",
    "    WHERE \n",
    "        year = 2023\n",
    "      AND month IN (4,5)\n",
    "      AND project_family = 'wikipedia'\n",
    ") a\n",
    "\n",
    "GROUP BY CONCAT(year,'-',LPAD(month,2,'0')) , region\n",
    "\n",
    "LIMIT 1000\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb21942-9a7c-41ef-a615-7154abb86d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_devices_by_region_aprilmay=wmf.spark.run(unique_devices_by_region_query_aprilmay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b523fcd-cec3-4899-a276-181abde39731",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_devices_by_region_aprilmay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd51ede-c001-4ca0-b979-31498dece4d5",
   "metadata": {},
   "source": [
    "## June 2023 unique devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06544f6a-0c3c-4fa6-bec7-9476e22181b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_devices_by_region_query_june='''\n",
    "SELECT\n",
    "    CONCAT(year,'-',LPAD(month,2,'0')) AS month,\n",
    "    region,\n",
    "    SUM(unique_devices) AS unique_devices\n",
    "FROM\n",
    "(\n",
    "        SELECT\n",
    "      year, month,\n",
    "      cr.wmf_region AS region,\n",
    "      uniques_estimate as unique_devices\n",
    "    FROM \n",
    "        wmf.unique_devices_per_project_family_monthly ud\n",
    "    LEFT JOIN gdi.country_meta_data cr \n",
    "    ON ud.country_code=cr.country_code_iso_2\n",
    "    WHERE \n",
    "        year = 2023\n",
    "      AND month = 6\n",
    "      AND project_family = 'wikipedia'\n",
    ") a\n",
    "\n",
    "GROUP BY CONCAT(year,'-',LPAD(month,2,'0')) , region\n",
    "\n",
    "LIMIT 1000\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416411ac-8bbc-4c2f-be3d-cb8a408ecce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_devices_by_region_june=wmf.spark.run(unique_devices_by_region_query_june)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5e3715-d66b-4236-a941-c39e39b7b512",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_devices_by_region_june"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de6e561-85f6-4248-98c9-0092847ba575",
   "metadata": {},
   "source": [
    "### July 2023 unique devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f095f9d1-026d-4d87-a0ec-6eef520c5621",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_devices_by_region_query_july='''\n",
    "SELECT\n",
    "    CONCAT(year,'-',LPAD(month,2,'0')) AS month,\n",
    "    region,\n",
    "    SUM(unique_devices) AS unique_devices\n",
    "FROM\n",
    "(\n",
    "        SELECT\n",
    "      year, month,\n",
    "      cr.wmf_region AS region,\n",
    "      uniques_estimate as unique_devices\n",
    "    FROM \n",
    "        wmf.unique_devices_per_project_family_monthly ud\n",
    "    LEFT JOIN gdi.country_meta_data cr \n",
    "    ON ud.country_code=cr.country_code_iso_2\n",
    "    WHERE \n",
    "        year = 2023\n",
    "      AND month = 7\n",
    "      AND project_family = 'wikipedia'\n",
    ") a\n",
    "\n",
    "GROUP BY CONCAT(year,'-',LPAD(month,2,'0')) , region\n",
    "\n",
    "LIMIT 1000\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140e39f8-470f-4dde-a95f-6a3c40e5ec80",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_devices_by_region_july=wmf.spark.run(unique_devices_by_region_query_july)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bbbb65-4a55-4216-a678-a8c385662add",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_devices_by_region_july"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f790ac-09dc-4eef-95e2-51804a93100f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wmfdata as wmf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e51833b-549e-415e-9267-0cb3c5f1704f",
   "metadata": {},
   "source": [
    "Looking at 2022-07 data to check for discrepancies since this 2022-07 is the first month for reliable YoY data. \n",
    "Context: When we did a YoY comparison of unique devices we found -8% drop which seemed high. When we looked at regional breakdown we saw increases in a lot of regions and unclassed category didn't account for the differences. Upon looking further we saw that 2022-07 unique devices calculated in 2022 vs now in 2023 was different. This was due to the data being fixed retroactively (for 2022-08 and 2022-09 snapshots) in unique_devices datasets due to the Overcount issue which was detected in August 2022 and resolved on Sep 2022. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d83807-da33-480c-8bd4-bb828fe58f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2022-07\n",
    "unique_devices_by_region_query_july2022='''\n",
    "SELECT\n",
    "    CONCAT(year,'-',LPAD(month,2,'0')) AS month,\n",
    "    region,\n",
    "    SUM(unique_devices) AS unique_devices\n",
    "FROM\n",
    "(\n",
    "        SELECT\n",
    "      year, month,\n",
    "      cr.wmf_region AS region,\n",
    "      uniques_estimate as unique_devices\n",
    "    FROM \n",
    "        wmf.unique_devices_per_project_family_monthly ud\n",
    "    LEFT JOIN gdi.country_meta_data cr \n",
    "    ON ud.country_code=cr.country_code_iso_2\n",
    "    WHERE \n",
    "        year = 2022\n",
    "      AND month = 7\n",
    "      AND project_family = 'wikipedia'\n",
    ") a\n",
    "\n",
    "GROUP BY CONCAT(year,'-',LPAD(month,2,'0')) , region\n",
    "\n",
    "LIMIT 1000\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e719ebc-8e65-4c70-bfea-6e8b89c67083",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_devices_by_region_july2022 = wmf.spark.run(unique_devices_by_region_query_july2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9e6e05-492b-4e56-95d4-c1bd6e9a3c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_devices_by_region_july2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675afdb5-fbc5-4c6f-9945-10ef616ea490",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_devices_july2022= wmf.spark.run('''\n",
    "SELECT\n",
    "    CONCAT(year,'-',LPAD(month,2,'0'),'-01') AS month,\n",
    "    SUM(unique_devices) AS unique_devices\n",
    "FROM\n",
    "(\n",
    "    SELECT\n",
    "       year, month,\n",
    "      uniques_estimate as unique_devices\n",
    "    FROM \n",
    "        wmf.unique_devices_per_project_family_monthly\n",
    "    WHERE \n",
    "        year >= 2014\n",
    "      AND project_family = 'wikipedia'\n",
    ") a\n",
    "\n",
    "WHERE  (year = 2022 AND month = 7)\n",
    "GROUP BY CONCAT(year,'-',LPAD(month,2,'0'),'-01')\n",
    "LIMIT 1000\n",
    "'''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0d0a90-4f94-4dbe-a11c-557437fab1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_devices_july2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a740f98b-8490-4c03-b6a8-e00cff8cc39b",
   "metadata": {},
   "source": [
    "In 2022-07 we had previously reported 1,648,975,154 unique devices. This is incorrect. The corrected unique devices for 2022-07 is 1,489,002,647"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0eb7d38-6c67-4575-b5c3-b968f8782cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_devices_aug2022= wmf.spark.run('''\n",
    "SELECT\n",
    "    CONCAT(year,'-',LPAD(month,2,'0'),'-01') AS month,\n",
    "    SUM(unique_devices) AS unique_devices\n",
    "FROM\n",
    "(\n",
    "    SELECT\n",
    "       year, month,\n",
    "      uniques_estimate as unique_devices\n",
    "    FROM \n",
    "        wmf.unique_devices_per_project_family_monthly\n",
    "    WHERE \n",
    "        year >= 2014\n",
    "      AND project_family = 'wikipedia'\n",
    ") a\n",
    "\n",
    "WHERE  (year = 2022 AND month > 6)\n",
    "GROUP BY CONCAT(year,'-',LPAD(month,2,'0'),'-01')\n",
    "LIMIT 1000\n",
    "'''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff297eea-d6a7-49a9-88d2-3e7f2f68ab6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_devices_aug2022.sort_values('month')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9a1973-b5b6-4bd1-b668-df6d3e4e47b5",
   "metadata": {},
   "source": [
    "This was part of testing done to verify the difference in unique devices between July 2022 and Aug 2022 vs July-Aug 2023. \n",
    "The data for July-Aug 2022 was fixed in September 2022 due to the [Unique Devices By Family Overcount](https://wikitech.wikimedia.org/wiki/Analytics/Data_Lake/Data_Issues/2021-02-09_Unique_Devices_By_Family_Overcount) issue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac2696b-9922-4c70-91e1-15797cabffc3",
   "metadata": {},
   "source": [
    "In 2022-08 we had previously reported 1,665,646,489 unique devices. This is incorrect. The corrected unique devices for 2022-08 is 1,542,342,588."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc313a43-b0fe-4133-ad99-addced48d813",
   "metadata": {},
   "source": [
    "### August 2023 unique devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "443fb03f-3b89-4ea0-90e6-1e3ef7020eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_devices_by_region_query_aug='''\n",
    "SELECT\n",
    "    CONCAT(year,'-',LPAD(month,2,'0')) AS month,\n",
    "    region,\n",
    "    SUM(unique_devices) AS unique_devices\n",
    "FROM\n",
    "(\n",
    "        SELECT\n",
    "      year, month,\n",
    "      cr.wmf_region AS region,\n",
    "      uniques_estimate as unique_devices\n",
    "    FROM \n",
    "        wmf.unique_devices_per_project_family_monthly ud\n",
    "    LEFT JOIN gdi.country_meta_data cr \n",
    "    ON ud.country_code=cr.country_code_iso_2\n",
    "    WHERE \n",
    "        year = 2023\n",
    "      AND month = 8\n",
    "      AND project_family = 'wikipedia'\n",
    ") a\n",
    "\n",
    "GROUP BY CONCAT(year,'-',LPAD(month,2,'0')) , region\n",
    "\n",
    "LIMIT 1000\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9597426e-2457-46ce-ad25-16226e5f0966",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "unique_devices_by_region_aug=wmf.spark.run(unique_devices_by_region_query_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a5abd57-240e-4310-bf2d-59234b6eea61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>region</th>\n",
       "      <th>unique_devices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-08</td>\n",
       "      <td>East, Southeast Asia, &amp; Pacific</td>\n",
       "      <td>272486471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-08</td>\n",
       "      <td>Middle East &amp; North Africa</td>\n",
       "      <td>67049240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-08</td>\n",
       "      <td>North America</td>\n",
       "      <td>309218036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-08</td>\n",
       "      <td>Latin America &amp; Caribbean</td>\n",
       "      <td>138582748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-08</td>\n",
       "      <td>Central &amp; Eastern Europe &amp; Central Asia</td>\n",
       "      <td>188744285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2023-08</td>\n",
       "      <td>South Asia</td>\n",
       "      <td>161663600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2023-08</td>\n",
       "      <td>Sub-Saharan Africa</td>\n",
       "      <td>35617019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2023-08</td>\n",
       "      <td>UNCLASSED</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2023-08</td>\n",
       "      <td>None</td>\n",
       "      <td>418008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2023-08</td>\n",
       "      <td>Northern &amp; Western Europe</td>\n",
       "      <td>355186148</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     month                                   region  unique_devices\n",
       "0  2023-08          East, Southeast Asia, & Pacific       272486471\n",
       "1  2023-08               Middle East & North Africa        67049240\n",
       "2  2023-08                            North America       309218036\n",
       "3  2023-08                Latin America & Caribbean       138582748\n",
       "4  2023-08  Central & Eastern Europe & Central Asia       188744285\n",
       "5  2023-08                               South Asia       161663600\n",
       "6  2023-08                       Sub-Saharan Africa        35617019\n",
       "7  2023-08                                UNCLASSED              33\n",
       "8  2023-08                                     None          418008\n",
       "9  2023-08                Northern & Western Europe       355186148"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_devices_by_region_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea873e65-717c-4971-83de-1b65e783a6e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
